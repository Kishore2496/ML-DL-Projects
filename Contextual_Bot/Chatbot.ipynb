{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQPmJVq4aDki"
      },
      "source": [
        "#Libraries needed for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "#Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "!pip install tflearn\n",
        "import json\n",
        "import random\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h-snFdHp8j2"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqwbbaNbrph9"
      },
      "source": [
        "#import our chat-bot intents file\n",
        "with open('intents.json') as json_data:\n",
        "  intents = json.load(json_data)\n",
        "intents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgBxpVvFsiqy"
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "#loop through each sentence in the intent's patterns\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each and every word in the sentence\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    #add words to the word list\n",
        "    words.extend(w)\n",
        "    #add word(s) to documents \n",
        "    documents.append((w, intent['tag']))\n",
        "    #add tags to class list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CjSz2-swZQ2"
      },
      "source": [
        "#Perform stemming and lower each word as well as remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "#remove duplicate classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), \", documents\")\n",
        "print(len(classes), \" classes\", classes)\n",
        "print(len(words), \" unique stemmed words\", words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VivhWMsY1WzI"
      },
      "source": [
        "documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9W8tiWEwbSb"
      },
      "source": [
        "#create training data\n",
        "training = []\n",
        "output = []\n",
        "#create empty array for output\n",
        "output_empty = [0] * len(classes) #empty array with 0 in every cell\n",
        "\n",
        "#create training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "  #initialize bag of words\n",
        "  bag = []\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  #stemming each word \n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  #create bag of words array\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "  \n",
        "  #output is 1 for current tag and '0' for rest of the other tags \n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "  #print(output_row)\n",
        "  training.append([bag, output_row]) \n",
        "\n",
        "#end of doc loop\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=\"object\")\n",
        "\n",
        "# creating training lists\n",
        "train_x = list(training[:,0]) \n",
        "train_y = list(training[:,1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6iu0wh82hvZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2BP7tb78FKV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}